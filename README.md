# Machine Learning Techniques, National Taiwan University, Spring 2020

**Lecturer: Hsuan-Tien Lin**

This repository stores some of my solution of the course *Machine Learning Techniques* offered in NTU and Coursera.

---

For the NTU version in Spring 2020, there are three assignments. ([See here for more information](https://www.csie.ntu.edu.tw/~htlin/course/mltech20spring/index.php))

**Homework 1:** 

Mainly focus on support vector machine (SVM), including hard/soft margin SVM, primal/dual problems, quadratic programming problems and kernel trick. There are python codes using *Sklearn* to train a SVM classifier. 

**Homework 2:**

Mainly focus on probabilistic SVM (SVM + logistic regression), neural network, deep learning, autoencoder. There are python codes using *Pytorch* to implement an autoencoder.

**Homework 3:**

Mainly focus on aggregation techniques, including blending, bagging, Adaboost and decision trees. There are no programming problems.

**Final Project:**

​The final project is a mango classification problem. Learners have to classify images with mangoes into three categories (A, B, C) according to their quality. We try three models to do such classification problems, including deep learning methods, SVM methods and random forest methods.

​Noises in the images and labels are the main difficulties in the task. The mangoes often lie at the corner of the images, rather than the center. Although this might not be a problem to methods like CNN, which extracts hidden features from the images, traditional methods needs data cleansing to acquire better performance. 

---

For the Coursera version, there are four assignments. ([See here for more information](https://www.coursera.org/learn/machine-learning-techniques))

**Homework 1:**

Same topics as Homework 1 in the NTU version.

**Homework 2:**

Mainly focus on kernel models and aggregating models, e.g, bagging, boosting. There are codes implementing Adaboost using Python.

**Homework 3:**

Mainly focus on decision trees, random forests, and gradient boosting (functional gradient descent). There are python codes using *Sklearn* to train a decision-tree classifier.

**Homework 4:**

Mainly focus on neural network and deep learning. I use *torch* to train a neural network. Besides, I also play with k-nearest neighbor algorithms and k-means algorithms.